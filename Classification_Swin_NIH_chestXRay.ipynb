{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d94699-0a27-4072-bb56-771e8d260b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))  # 0 indicates the first GPU if you have multiple\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2937e9d8-90a3-4e62-8532-9e53999e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image  # Import PIL's Image modul\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774918cc-c810-4c7e-86e5-4f22d45b2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data.zip to /scratch/atricham\n",
      "100%|█████████████████████████████████████▉| 42.0G/42.0G [10:02<00:00, 74.7MB/s]\n",
      "100%|██████████████████████████████████████| 42.0G/42.0G [10:02<00:00, 74.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d nih-chest-xrays/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac295fc-8f80-418b-a787-053f7be3d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source directories\n",
    "source_directories = [\"images_003/images\", \"images_004/images\",\n",
    "                     \"images_005/images\", \"images_007/images\",\n",
    "                     \"images_001/images\", \"images_002/images\",\n",
    "                     \"images_006/images\", \"images_008/images\",\n",
    "                     \"images_009/images\", \"images_010/images\",\n",
    "                     \"images_011/images\", \"images_012/images\"]\n",
    "\n",
    "# Define the destination directory\n",
    "destination_directory = \"image_folder/images\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through the source directories\n",
    "for source_directory in source_directories:\n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_directory)\n",
    "    \n",
    "    # Move or copy each file to the destination directory\n",
    "    for file in files:\n",
    "        source_path = os.path.join(source_directory, file)\n",
    "        destination_path = os.path.join(destination_directory, file)\n",
    "        # Use shutil.move() to move or shutil.copy() to copy\n",
    "        shutil.move(source_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59838f1d-49c1-4a40-841d-7da9d25d85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define Swin Transformer Blocks\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size, mlp_ratio=4):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_ratio * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention and MLP layers\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x, x, x)[0] + residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x) + residual\n",
    "        return x\n",
    "\n",
    "# Define Swin Transformer\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, num_layers, num_heads, mlp_ratio=4):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        dim = 96  # Adjust the dimension as needed for your task (e.g., 96, 192, 384)\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embed = nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "        # Swin Transformer Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim, num_heads, window_size=(image_size // patch_size), mlp_ratio=mlp_ratio)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification Head\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and flatten\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Swin Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example Usage:\n",
    "image_size = 256  # Adjust as needed\n",
    "patch_size = 4   # Adjust as needed\n",
    "num_classes = 14  # Number of classes for multi-label classification\n",
    "num_layers = 12   # Number of layers in the Swin Transformer\n",
    "num_heads = 8     # Number of attention heads\n",
    "mlp_ratio = 4     # MLP expansion ratio\n",
    "\n",
    "model = SwinTransformer(image_size, patch_size, num_classes, num_layers, num_heads, mlp_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e440c99c-f7a0-4d6f-90bd-89b17f8a1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # Replace with your loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Replace with your optimizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c03e816f-0cf6-4ea2-9db7-794c47b20da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        # Check the number of channels in the image\n",
    "        if image.mode == 'RGB':\n",
    "            # Handle RGB-D images by converting them to grayscale\n",
    "            image = image.convert('L')\n",
    "        elif image.mode != 'L':\n",
    "            # Handle other image modes (e.g., RGBA) by converting them to grayscale\n",
    "            image = image.convert('L')\n",
    "\n",
    "        labels = self.data.iloc[idx, 1:].values.astype('float32')  # Assuming labels start from the second column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e94ec51-086e-44ef-a4b1-40eee27b8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the paths to your CSV files\n",
    "train_csv = \"train.csv\"\n",
    "val_csv = \"validation.csv\"\n",
    "test_csv = \"test.csv\"\n",
    "\n",
    "# Define the path to your image folder\n",
    "image_folder_path = \"image_folder/images\"\n",
    "\n",
    "# Define your data transformations\n",
    "transform = transforms.Compose([  # Define your data transformations here\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485], [0.224]),\n",
    "    # Add more transformations as needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88091b92-dcd6-4a2e-b311-e9eac8d800e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create custom datasets for each split\n",
    "train_dataset = CustomDataset(train_csv, image_folder_path, transform=transform)\n",
    "val_dataset = CustomDataset(val_csv, image_folder_path, transform=transform)\n",
    "test_dataset = CustomDataset(test_csv, image_folder_path, transform=transform)\n",
    "\n",
    "# Define batch sizes\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "\n",
    "# Create DataLoader objects for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size , shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350a840-90e2-4079-b261-b5417c951f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 2170/2170 [1:56:52<00:00,  3.23s/it, Train Loss=0.145]  \n",
      "Epoch 1/50: 100%|██████████| 534/534 [30:58<00:00,  3.48s/it, Val Loss=0.104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1, Epoch [1/50] - Average Train Loss: 0.1644 - Average Val Loss: 0.1642 - Train AUC: 0.5214 - Val AUC: 0.5590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 2170/2170 [1:44:08<00:00,  2.88s/it, Train Loss=0.116]  \n",
      "Epoch 2/50: 100%|██████████| 534/534 [24:57<00:00,  2.80s/it, Val Loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/1, Epoch [2/50] - Average Train Loss: 0.1629 - Average Val Loss: 0.1641 - Train AUC: 0.5359 - Val AUC: 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:  42%|████▏     | 915/2170 [56:46<1:06:39,  3.19s/it, Train Loss=0.138] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define the number of training and validation epochs\n",
    "num_epochs = 50\n",
    "num_runs = 1  # Number of times to run the training loop\n",
    "\n",
    "# Lists to store training and validation loss for plotting\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# Lists to store AUC values for train, val, and test\n",
    "train_auc_history = []\n",
    "val_auc_history = []\n",
    "test_auc_history = []\n",
    "\n",
    "# Outer loop for multiple runs\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "\n",
    "    # Training and validation loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "\n",
    "        train_loader_with_progress = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader_with_progress):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "            # Print the training loss for each batch\n",
    "            train_loader_with_progress.set_postfix({\"Train Loss\": loss.item()})\n",
    "\n",
    "        average_train_loss = running_train_loss / len(train_loader)\n",
    "        train_loss_history.append(average_train_loss)\n",
    "\n",
    "        # Calculate AUC for training data\n",
    "        train_auc = roc_auc_score(train_labels, train_predictions)\n",
    "        train_auc_history.append(train_auc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "\n",
    "        val_loader_with_progress = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(val_loader_with_progress):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                val_predictions.extend(outputs.cpu().detach().numpy())\n",
    "                val_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "                # Print the validation loss for each batch during validation\n",
    "                val_loader_with_progress.set_postfix({\"Val Loss\": loss.item()})\n",
    "\n",
    "        average_val_loss = running_val_loss / len(val_loader)\n",
    "        val_loss_history.append(average_val_loss)\n",
    "\n",
    "\n",
    "        # Calculate AUC for validation data (your existing code)\n",
    "        val_auc = roc_auc_score(val_labels, val_predictions)\n",
    "        val_auc_history.append(val_auc)\n",
    "\n",
    "        # Print the average training and validation loss for the epoch\n",
    "        print(f\"Run {run + 1}/{num_runs}, Epoch [{epoch + 1}/{num_epochs}] - Average Train Loss: {average_train_loss:.4f} - Average Val Loss: {average_val_loss:.4f} - Train AUC: {train_auc:.4f} - Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "\n",
    "    # Calculate AUC for test data\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            test_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "    test_auc = roc_auc_score(test_labels, test_predictions)\n",
    "    test_auc_history.append(test_auc)\n",
    "    print(f\"Run {run + 1}/{num_runs} - Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # Save your trained model if needed (you can save it with a unique name for each run)\n",
    "    torch.save(model.state_dict(), f\"model_full_extend{run + 1}.pth\")\n",
    "\n",
    "# Plot and save the AUC values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_runs + 1), test_auc_history, marker='o', linestyle='-')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Test AUC')\n",
    "plt.title('Test AUC for Multiple Runs')\n",
    "plt.grid(True)\n",
    "plt.savefig('test_auc_plot_scrtatch.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
